Commit ID: 7e39e92e8e00b02f19d2406bd06f928a7a0ea18c



On branch master
Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   README.md
	deleted:    clash.tar.gz
	deleted:    clash/config.yaml
	deleted:    clash/mihomo
	modified:   eval.py
	modified:   src/config.py
	modified:   src/constants.py
	modified:   src/data.py
	modified:   src/model/est_coord.py
	modified:   src/model/est_pose.py
	modified:   src/robot/cfg.py
	modified:   src/utils.py
	modified:   test.sh
	modified:   train.py
	deleted:    train.sh

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	data_generated.zip
	data_generated/
	eval.sh
	src/pc_box.py
	train_part1.sh
	train_part2.sh

no changes added to commit (use "git add" and/or "git commit -a")


diff --git a/README.md b/README.md
index ca94902..d9b0af9 100644
--- a/README.md
+++ b/README.md
@@ -7,7 +7,7 @@ In this assignment, you are required to grasp a known object (power drill). We p
 You can install the environment as follows:
 
 ```sh
-conda create -n hw2 python=3.10
+conda create -n hw2 python=3.10v
 conda activate hw2
 conda install conda-forge::roboticstoolbox-python==1.0.3
 
@@ -72,3 +72,4 @@ If you install some packages that are not in the ```Environment``` part, please
 You will get 1% bonus on the final score if you have any model with success rate >= 92.5%
 
 When the point cloud is noisy, you might not be able to fit well if you simply fit using all object points due to the outliers. We can use RANSAC to solve this problem, and you will get 2% bonus on the final score if you implement RANSAC in Part II using only numpy or torch.
+
diff --git a/clash.tar.gz b/clash.tar.gz
deleted file mode 100644
index 7bbda50..0000000
Binary files a/clash.tar.gz and /dev/null differ
diff --git a/clash/config.yaml b/clash/config.yaml
deleted file mode 100644
index 4fddfdd..0000000
--- a/clash/config.yaml
+++ /dev/null
@@ -1,21 +0,0 @@
-mixed-port: 9888
-log-level: info
-mode: Rule
-dns:
-    enabled: true
-    listen: '0.0.0.0:1053'
-    ipv6: false
-    default-nameserver: [223.5.5.5, 114.114.114.114]
-    nameserver: [223.5.5.5, 114.114.114.114]
-proxies:
-    - { name: 'ðŸ‡ºðŸ‡¸ ç¾Žå›½ 01', type: vless, server: 162.159.153.19, port: 443, uuid: 19f9e525-d435-4534-b37a-681ca60d0ec0, udp: true, tls: true, skip-cert-verify: false, flow: '', client-fingerprint: chrome, network: ws, ws-opts: { path: /pq/us1, headers: { Host: us1.pqjc.buzz } } }
-    - { name: 'ðŸ‡ºðŸ‡¸ ç¾Žå›½ 02', type: vless, server: 104.19.36.176, port: 443, uuid: 19f9e525-d435-4534-b37a-681ca60d0ec0, udp: true, tls: true, skip-cert-verify: false, flow: '', client-fingerprint: chrome, network: ws, ws-opts: { path: /pq/us3, headers: { Host: us3.pqjc.buzz } } }
-    - { name: 'ðŸ‡ºðŸ‡¸ ç¾Žå›½ 03', server: 192.9.130.120, port: 35000, ports: 35000-39000, mport: 35000-39000, udp: true, skip-cert-verify: true, sni: www.apple.com, type: hysteria2, password: 19f9e525-d435-4534-b37a-681ca60d0ec0 }
-    - { name: 'ðŸ‡®ðŸ‡³ å°åº¦ 01', type: vless, server: 162.159.152.116, port: 443, uuid: 19f9e525-d435-4534-b37a-681ca60d0ec0, udp: true, tls: true, skip-cert-verify: false, flow: '', client-fingerprint: chrome, network: ws, ws-opts: { path: /pq/in1, headers: { Host: in1.xn--ghqu5fm27b67w.com } } }
-
-proxy-groups:
-    - { name: 'Auto', type: url-test, url: 'http://cp.cloudflare.com/generate_204', interval: 300, proxies: [ðŸ‡ºðŸ‡¸ ç¾Žå›½ 01, ðŸ‡ºðŸ‡¸ ç¾Žå›½ 02, ðŸ‡ºðŸ‡¸ ç¾Žå›½ 03, ðŸ‡®ðŸ‡³ å°åº¦ 01] }
-
-rules:
-  - 'MATCH,Auto'
-  
diff --git a/clash/mihomo b/clash/mihomo
deleted file mode 100755
index db42498..0000000
Binary files a/clash/mihomo and /dev/null differ
diff --git a/eval.py b/eval.py
index 02f23d7..b46d215 100644
--- a/eval.py
+++ b/eval.py
@@ -69,6 +69,8 @@ def main():
         )
 
         with torch.no_grad():
+            print(torch.from_numpy(dic["pc"])[None].to(args.device).shape)
+            input()
             est_trans, est_rot = model.est(
                 torch.from_numpy(dic["pc"])[None].to(args.device)
             )
diff --git a/src/config.py b/src/config.py
index 1aaf852..192cad6 100644
--- a/src/config.py
+++ b/src/config.py
@@ -17,11 +17,11 @@ class Config:
     """the robot we are using"""
     obj_name: str = "power_drill"
     """the object we want to grasp"""
-    checkpoint: str = None
+    checkpoint: str = None # '/home/secure/Codes/lzy/eai/Assignment2/exps/est_pose_PointNet_euler_1e-3_64/checkpoint/checkpoint_10000.pth'
     """if not None, then we will continue training from this checkpoint"""
-    max_iter: int = 10000
+    max_iter: int = 20000
     """the maximum number of iterations"""
-    batch_size: int = 16
+    batch_size: int = 64
     """the batch size for training"""
     learning_rate: float = 1e-3
     """maximum (and initial) learning rates"""
diff --git a/src/constants.py b/src/constants.py
index e30380b..78b505c 100644
--- a/src/constants.py
+++ b/src/constants.py
@@ -4,9 +4,9 @@ import numpy as np
 DEPTH_IMG_SCALE = 16384
 
 # simulation initialization settings
-TABLE_HEIGHT = 0.5
-OBJ_INIT_TRANS = np.array([0.45, 0.2, 0.6])
-OBJ_RAND_RANGE = 0.3
+TABLE_HEIGHT = 0.74
+OBJ_INIT_TRANS = np.array([0.51, 0.365, 0.82])
+OBJ_RAND_RANGE = 1.3
 OBJ_RAND_SCALE = 0.05
 
 # clip the point cloud to a box
@@ -14,13 +14,13 @@ PC_MIN = np.array(
     [
         OBJ_INIT_TRANS[0] - OBJ_RAND_RANGE / 2,
         OBJ_INIT_TRANS[1] - OBJ_RAND_RANGE / 2,
-        0.505,
+        0.745,
     ]
 )
 PC_MAX = np.array(
     [
         OBJ_INIT_TRANS[0] + OBJ_RAND_RANGE / 2,
         OBJ_INIT_TRANS[1] + OBJ_RAND_RANGE / 2,
-        0.65,
+        0.895,
     ]
 )
diff --git a/src/data.py b/src/data.py
index 05c7116..24ef27a 100644
--- a/src/data.py
+++ b/src/data.py
@@ -30,7 +30,7 @@ class PoseDataset(Dataset):
         super().__init__()
         self.config = config
         self.robot_cfg = get_robot_cfg(config.robot)
-        self.data_root = os.path.join("data", config.obj_name, mode)
+        self.data_root = os.path.join("data_generated", config.obj_name, mode)
         self.files = sorted(os.listdir(self.data_root))
         self.files = self.files * scale
         random.shuffle(self.files)
@@ -104,8 +104,13 @@ class PoseDataset(Dataset):
             full_coord = np.einsum(
                 "ba,nb->na", obj_pose[:3, :3], full_pc_world - obj_pose[:3, 3]
             )
+            
 
-            pc_mask = get_workspace_mask(full_pc_world)
+            pc_mask = get_workspace_mask(full_pc_world, obj_pose)
+            # valid_points = np.sum(pc_mask)
+            # if valid_points == 0:
+            #     print(f"Warning: No valid points in {fdir}, skipping sample.")
+            #     return self.__getitem__()  # è·³è¿‡æ— æ•ˆæ ·æœ¬
             sel_pc_idx = np.random.randint(0, np.sum(pc_mask), self.config.point_num)
 
             pc_camera = full_pc_camera[pc_mask][sel_pc_idx]
diff --git a/src/model/est_coord.py b/src/model/est_coord.py
index 0487181..deb315f 100644
--- a/src/model/est_coord.py
+++ b/src/model/est_coord.py
@@ -2,11 +2,13 @@ from typing import Tuple, Dict
 import numpy as np
 import torch
 from torch import nn
+import torch.nn.functional as F
 
 from ..config import Config
 from ..vis import Vis
 
 
+
 class EstCoordNet(nn.Module):
 
     config: Config
@@ -17,7 +19,24 @@ class EstCoordNet(nn.Module):
         """
         super().__init__()
         self.config = config
-        raise NotImplementedError("You need to implement some modules here")
+
+        self.mlp1 = nn.Sequential(nn.Linear(3, 64), nn.ReLU())
+        self.mlp2 = nn.Sequential(nn.Linear(64, 128), nn.ReLU())
+        self.mlp3 = nn.Sequential(nn.Linear(128, 256), nn.ReLU())
+        
+        self.global_fc = nn.Sequential(
+            nn.Linear(256, 256),
+            nn.ReLU(),
+            nn.Linear(256, 128)
+        )
+
+        self.decoder = nn.Sequential(
+            nn.Linear(64 + 128 + 128, 128),  
+            nn.ReLU(),
+            nn.Linear(128, 64),
+            nn.ReLU(),
+            nn.Linear(64, 3)
+        )
 
     def forward(
         self, pc: torch.Tensor, coord: torch.Tensor, **kwargs
@@ -39,13 +58,88 @@ class EstCoordNet(nn.Module):
         Dict[str, float]
             A dictionary containing additional metrics you want to log
         """
-        raise NotImplementedError("You need to implement the forward function")
-        loss = ...
-        metric = dict(
-            loss=loss,
-            # additional metrics you want to log
-        )
-        return loss, metric
+        B, N, _ = pc.shape
+
+        x1 = self.mlp1(pc)    
+        x2 = self.mlp2(x1)   
+        x3 = self.mlp3(x2)     
+
+        x_max = torch.max(x3, dim=1, keepdim=True)[0]  
+        x_global = self.global_fc(x_max)               
+        x_global_expand = x_global.expand(-1, N, -1)  
+
+        x_cat = torch.cat([x1, x2, x_global_expand], dim=-1)  
+
+        pred_coord = self.decoder(x_cat) 
+
+        loss = F.mse_loss(pred_coord, coord)
+
+        return loss, {'loss_coord': loss.item()}
+    
+    def umeyama_alignment(self, src: torch.Tensor, tgt: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
+        B, N, _ = src.shape
+
+        mu_src = src.mean(dim=1, keepdim=True) 
+        mu_tgt = tgt.mean(dim=1, keepdim=True) 
+
+        src_centered = src - mu_src  
+        tgt_centered = tgt - mu_tgt
+
+        cov = torch.matmul(tgt_centered.transpose(1, 2), src_centered) / N 
+
+        U, S, Vt = torch.linalg.svd(cov)
+
+        d = torch.det(torch.matmul(U, Vt))
+        D = torch.eye(3, device=src.device).unsqueeze(0).repeat(B, 1, 1)
+        D[:, 2, 2] = torch.where(d < 0, -1.0, 1.0)
+
+        R = torch.matmul(torch.matmul(U, D), Vt) 
+        t = mu_tgt.squeeze(1) - torch.matmul(R, mu_src.squeeze(1).unsqueeze(-1)).squeeze(-1) 
+
+        return R, t
+
+    
+    def ransac_fit(self, pc: torch.Tensor, pred_coord: torch.Tensor, max_iters=100, threshold=0.01):
+        B, N, _ = pc.shape
+        device = pc.device
+        best_R = torch.eye(3, device=device).unsqueeze(0).repeat(B, 1, 1)
+        best_t = torch.zeros(B, 3, device=device)
+        best_inlier_count = torch.zeros(B, dtype=torch.long, device=device)
+
+        for _ in range(max_iters):
+            idx = torch.randint(0, N, (B, 3), device=device)  
+            src = torch.gather(pred_coord, 1, idx.unsqueeze(-1).repeat(1, 1, 3))
+            tgt = torch.gather(pc, 1, idx.unsqueeze(-1).repeat(1, 1, 3))
+
+            R, t = self.umeyama_alignment(src, tgt)
+
+            pred_pc = (pred_coord @ R.transpose(1, 2)) + t.unsqueeze(1)
+            dist = torch.norm(pred_pc - pc, dim=-1) 
+            inlier_mask = (dist < threshold)
+            inlier_count = inlier_mask.sum(dim=-1)
+
+            update_mask = inlier_count > best_inlier_count
+            best_inlier_count = torch.where(update_mask, inlier_count, best_inlier_count)
+            best_R = torch.where(update_mask.view(-1, 1, 1), R, best_R)
+            best_t = torch.where(update_mask.view(-1, 1), t, best_t)
+
+        return best_t, best_R
+    
+    def forward_coord_only(self, pc: torch.Tensor) -> torch.Tensor:
+        B, N, _ = pc.shape
+
+        x1 = self.mlp1(pc)    
+        x2 = self.mlp2(x1)  
+        x3 = self.mlp3(x2)   
+
+        x_max = torch.max(x3, dim=1, keepdim=True)[0] 
+        x_global = self.global_fc(x_max)              
+        x_global_expand = x_global.expand(-1, N, -1) 
+
+        x_cat = torch.cat([x1, x2, x_global_expand], dim=-1) 
+        pred_coord = self.decoder(x_cat)                     
+
+        return pred_coord
 
     def est(self, pc: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
         """
@@ -71,4 +165,8 @@ class EstCoordNet(nn.Module):
 
         The only requirement is that the input and output should be torch tensors on the same device and with the same dtype.
         """
-        raise NotImplementedError("You need to implement the est function")
+        self.eval()
+        with torch.no_grad():
+            pred_coord = self.forward_coord_only(pc)  
+            trans, rot = self.ransac_fit(pc, pred_coord)
+        return trans, rot
diff --git a/src/model/est_pose.py b/src/model/est_pose.py
index cc55de0..9fec946 100644
--- a/src/model/est_pose.py
+++ b/src/model/est_pose.py
@@ -1,22 +1,55 @@
 from typing import Tuple, Dict
 import torch
 from torch import nn
+import torch.nn.functional as F
 
 from ..config import Config
 
+def rotation_6d_to_matrix(d6: torch.Tensor) -> torch.Tensor:
+    a1 = d6[..., 0:3]
+    a2 = d6[..., 3:6]
+
+    b1 = F.normalize(a1, dim=-1)
+    b2 = F.normalize(a2 - (b1 * a2).sum(-1, keepdim=True) * b1, dim=-1)
+    b3 = torch.cross(b1, b2, dim=-1)
+
+    return torch.stack((b1, b2, b3), dim=-2)
+
+def geodesic_loss(R_pred: torch.Tensor, R_gt: torch.Tensor) -> torch.Tensor:
+    R_diff = torch.matmul(R_pred.transpose(-1, -2), R_gt)
+    trace = R_diff[..., 0, 0] + R_diff[..., 1, 1] + R_diff[..., 2, 2]
+    cos_theta = (trace - 1) / 2
+    cos_theta = torch.clamp(cos_theta, -1.0 + 1e-6, 1.0 - 1e-6)
+    theta = torch.acos(cos_theta)
+    return theta.mean()
 
 class EstPoseNet(nn.Module):
 
     config: Config
 
     def __init__(self, config: Config):
-        """
-        Directly estimate the translation vector and rotation matrix.
-        """
         super().__init__()
         self.config = config
-        raise NotImplementedError("You need to implement some modules here")
-
+        self.feat = nn.Sequential( 
+            nn.Conv1d(3, 64, 1),
+            nn.BatchNorm1d(64),
+            nn.ReLU(),
+            nn.Conv1d(64, 128, 1),
+            nn.BatchNorm1d(128),
+            nn.ReLU(),
+            nn.Conv1d(128, 1024, 1),
+            nn.BatchNorm1d(1024),
+            nn.ReLU()
+        )
+        self.fc = nn.Sequential(
+            nn.Linear(1024, 512),
+            nn.ReLU(),
+            nn.Linear(512, 256),
+            nn.ReLU()
+        )
+        self.trans_head = nn.Linear(256, 3) 
+        self.rot6d_head = nn.Linear(256, 6) 
+        
     def forward(
         self, pc: torch.Tensor, trans: torch.Tensor, rot: torch.Tensor, **kwargs
     ) -> Tuple[float, Dict[str, float]]:
@@ -39,11 +72,24 @@ class EstPoseNet(nn.Module):
         Dict[str, float]
             A dictionary containing additional metrics you want to log
         """
-        raise NotImplementedError("You need to implement the forward function")
-        loss = ...
+        pc = pc.transpose(1, 2)
+        feat = self.feat(pc)  
+        global_feat = torch.max(feat, 2)[0]  
+        x = self.fc(global_feat) 
+
+        pred_trans = self.trans_head(x)  
+        pred_rot6d = self.rot6d_head(x)  
+        pred_rot = rotation_6d_to_matrix(pred_rot6d)  
+
+        loss_trans = F.mse_loss(pred_trans, trans)
+        loss_rot = geodesic_loss(pred_rot, rot)
+
+        loss = loss_trans + loss_rot
+
         metric = dict(
-            loss=loss,
-            # additional metrics you want to log
+            loss=loss.item(),
+            loss_trans=loss_trans.item(),
+            loss_rot=loss_rot.item()
         )
         return loss, metric
 
@@ -67,4 +113,14 @@ class EstPoseNet(nn.Module):
         ----
         The rotation matrix should satisfy the requirement of orthogonality and determinant 1.
         """
-        raise NotImplementedError("You need to implement the est function")
+        pc = pc.transpose(1, 2)
+        feat = self.feat(pc)  
+        global_feat = torch.max(feat, 2)[0] 
+        x = self.fc(global_feat) 
+
+        pred_trans = self.trans_head(x)  
+        pred_rot6d = self.rot6d_head(x)  
+        pred_rot = rotation_6d_to_matrix(pred_rot6d) 
+
+        return pred_trans, pred_rot
+    
\ No newline at end of file
diff --git a/src/robot/cfg.py b/src/robot/cfg.py
index 2e93fd7..50f757a 100644
--- a/src/robot/cfg.py
+++ b/src/robot/cfg.py
@@ -57,10 +57,25 @@ class RobotCfg:
         return False
 
 
+# WRIST_CAMERA = CameraCfg(
+#     width=640,
+#     height=360,
+#     intrinsics=np.array([[326, 0, 320], [0, 326, 180], [0, 0, 1]]),
+#     extrinsics=np.array(
+#         [
+#             [0, 0, 1, 0],
+#             [0, -1, 0, 0.0655],
+#             [1, 0, 0, 0],
+#             [0.0, 0.0, 0.0, 1.0],
+#         ]
+#     ),
+#     link_name="left_arm_end_effector_mount_link",
+# )
+
 WRIST_CAMERA = CameraCfg(
-    width=640,
-    height=360,
-    intrinsics=np.array([[326, 0, 320], [0, 326, 180], [0, 0, 1]]),
+    width=1280,
+    height=720,
+    intrinsics=np.array([[649.5874633789062, 0, 644.9450073242188], [0, 648.9038696289062, 355.34857177734375], [0, 0, 1]]),
     extrinsics=np.array(
         [
             [0, 0, 1, 0],
diff --git a/src/utils.py b/src/utils.py
index c4d3bf3..1060e5d 100644
--- a/src/utils.py
+++ b/src/utils.py
@@ -81,7 +81,18 @@ def transform_grasp_pose(
     Grasp
         The transformed grasp in the robot frame.
     """
-    raise NotImplementedError
+    grasp_trans_cam = est_rot @ grasp.trans + est_trans
+    grasp_rot_cam = est_rot @ grasp.rot
+
+    grasp_trans_robot = cam_rot @ grasp_trans_cam + cam_trans
+    grasp_rot_robot = cam_rot @ grasp_rot_cam
+
+    return Grasp(
+        trans=grasp_trans_robot,
+        rot=grasp_rot_robot,
+        width=grasp.width,
+    )
+
 
 def get_pc(depth: np.ndarray, intrinsics: np.ndarray) -> np.ndarray:
     """
@@ -123,15 +134,26 @@ def get_pc(depth: np.ndarray, intrinsics: np.ndarray) -> np.ndarray:
     return points.T
 
 
-def get_workspace_mask(pc: np.ndarray) -> np.ndarray:
+def get_workspace_mask(pc: np.ndarray, obj_pose: np.ndarray, z_range: tuple = (-0.1, 0.1)) -> np.ndarray:
     """Get the mask of the point cloud in the workspace."""
+    # pc_mask = (
+    #     (pc[:, 0] > PC_MIN[0])
+    #     & (pc[:, 0] < PC_MAX[0])
+    #     & (pc[:, 1] > PC_MIN[1])
+    #     & (pc[:, 1] < PC_MAX[1])
+    #     & (pc[:, 2] > PC_MIN[2])
+    #     & (pc[:, 2] < PC_MAX[2])
+    # )
+    obj_z = obj_pose[2, 3]
+    z_min = obj_z + z_range[0]
+    z_max = obj_z + z_range[1]
     pc_mask = (
         (pc[:, 0] > PC_MIN[0])
         & (pc[:, 0] < PC_MAX[0])
         & (pc[:, 1] > PC_MIN[1])
         & (pc[:, 1] < PC_MAX[1])
-        & (pc[:, 2] > PC_MIN[2])
-        & (pc[:, 2] < PC_MAX[2])
+        & (pc[:, 2] > z_min)
+        & (pc[:, 2] < z_max)
     )
     return pc_mask
 
diff --git a/test.sh b/test.sh
index 9e505df..6f5d06c 100644
--- a/test.sh
+++ b/test.sh
@@ -5,9 +5,11 @@ export WANDB_API_KEY=5bdc90c568050775a6d10650e64857fbbc76742e
 export WANDB_USER_EMAIL=liumail2023@126.com
 export WANDB_USERNAME=liumail2023
 
-CKPT_PATH=""
+CKPT_PATH="/home/secure/Codes/lzy/eai/Assignment2/exps/est_pose_PointNet_euler_geodesic_1e-3_64/checkpoint/checkpoint_20000.pth"
 
 python test.py \
     --checkpoint ${CKPT_PATH} \
     --mode val \
-    --device cuda:2
\ No newline at end of file
+    --device cuda:3
+
+
diff --git a/train.py b/train.py
index 6f3f365..bc7601c 100644
--- a/train.py
+++ b/train.py
@@ -59,6 +59,8 @@ def main():
 
     # loading datasets
     train_dataset = PoseDataset(config, mode="train", scale=100)
+    print(train_dataset.__len__(), "train samples")
+    input("Press Enter to continue...")
     val_dataset = PoseDataset(config, mode="val", scale=100)
     # This loader will load data infinitely
     train_loader = Loader(
@@ -90,10 +92,12 @@ def main():
     scheduler = CosineAnnealingLR(
         optimizer, config.max_iter, eta_min=config.learning_rate_min
     )
+    
+    model.to(device)
 
     # load checkpoint if exists
     if config.checkpoint is not None:
-        checkpoint = torch.load(config.checkpoint, map_location="cpu")
+        checkpoint = torch.load(config.checkpoint, map_location=device)
         model.load_state_dict(checkpoint["model"])
         optimizer.load_state_dict(checkpoint["optimizer"])
         cur_iter = checkpoint["iter"]
@@ -104,7 +108,7 @@ def main():
         cur_iter = 0
 
     # init training
-    model.to(device)
+    
     model.train()
 
     # start training loop here
@@ -146,7 +150,10 @@ def main():
                     result_dicts.append(result_dict)
                 logger.log(
                     {
-                        k: np.array([dic[k].cpu() for dic in result_dicts]).mean()
+                        k: np.array([
+                            dic[k].cpu() if isinstance(dic[k], torch.Tensor) else dic[k]
+                            for dic in result_dicts
+                        ]).mean()
                         for k in result_dicts[0].keys()
                     },
                     "val",
diff --git a/train.sh b/train.sh
deleted file mode 100644
index 96865b9..0000000
--- a/train.sh
+++ /dev/null
@@ -1,14 +0,0 @@
-
-
-
-export WANDB_API_KEY=5bdc90c568050775a6d10650e64857fbbc76742e
-export WANDB_USER_EMAIL=liumail2023@126.com
-export WANDB_USERNAME=liumail2023
-
-MODEL_TYPE=est_pose # 'est_pose' 'est_coord'
-EXP_NAME=est_pose_test
-
-python train.py \
-    --model_type ${MODEL_TYPE} \
-    --exp_name ${EXP_NAME} \
-    --device cuda:2
\ No newline at end of file


